{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Watermarking System - Main Notebook\n",
        "\n",
        "This notebook provides a complete workflow for:\n",
        "1. Setting up the environment\n",
        "2. Generating watermarked datasets\n",
        "3. Generating tampered/attacked datasets\n",
        "4. Training ML-based watermarking models (Phase 3)\n",
        "5. Running verification and robustness tests\n",
        "6. Generating evaluation reports\n",
        "\n",
        "## Setup Instructions\n",
        "\n",
        "### For Google Colab Users:\n",
        "1. **Mount Google Drive** - The setup cell will do this automatically\n",
        "2. **Set BASE_DIR** - Already configured for `/content/drive/MyDrive/project_codes/models_new`\n",
        "3. **Upload your project** - Make sure your project files are in the BASE_DIR on Google Drive\n",
        "\n",
        "### Folder Structure:\n",
        "- `dataset/originals/` - Place your original images here (should be ready and can be empty initially)\n",
        "- `dataset/watermarked/` - Will be created automatically (can be empty or will be populated)\n",
        "- `dataset/tampered/` - Will be created automatically (can be empty or will be populated)\n",
        "- `dataset/metadata/` - Will be created automatically\n",
        "- `outputs/` - Will be created automatically for checkpoints, logs, and reports\n",
        "\n",
        "**Yes, you should have these folders ready (they can be empty) - the code will populate them!**\n",
        "\n",
        "### Important:\n",
        "- **Run cells in order** - Especially Step 1 (Setup) before any other steps\n",
        "- **Check BASE_DIR** - Make sure it points to your project root in Step 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ö†Ô∏è IMPORTANT: NumPy Compatibility Fix\n",
        "\n",
        "**Dependency Warnings Are Expected:**\n",
        "- You'll see warnings about numpy version conflicts (e.g., opencv wants numpy>=2.0, but we need numpy 1.x for scipy)\n",
        "- **These warnings are OK** - the code will still work despite the warnings\n",
        "- OpenCV 4.8.1 works fine with numpy 1.26.4, even though pip shows warnings\n",
        "\n",
        "**If you get numpy/scipy import errors:**\n",
        "1. Run Cell 4 (Install Dependencies) below\n",
        "2. **RESTART THE RUNTIME** (Runtime ‚Üí Restart runtime)\n",
        "3. Run Cell 4 again\n",
        "4. Continue with the rest\n",
        "\n",
        "**Why this happens:**\n",
        "- numpy 2.x is incompatible with scipy 1.11.x\n",
        "- Some packages (opencv 4.12+, pytensor, jax) require numpy 2.x\n",
        "- We force numpy 1.x for compatibility with scipy\n",
        "- OpenCV 4.8.1 works with numpy 1.x despite pip warnings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QUICK FIX: Run this if you're getting numpy/scipy import errors\n",
        "# This will force reinstall compatible versions\n",
        "\n",
        "print(\"Force fixing numpy/scipy compatibility...\")\n",
        "!pip uninstall -y numpy scipy\n",
        "!pip install --no-cache-dir \"numpy==1.26.4\" \"scipy==1.11.4\"\n",
        "\n",
        "# Test import\n",
        "try:\n",
        "    import numpy as np\n",
        "    import scipy\n",
        "    print(f\"‚úì numpy {np.__version__} - OK\")\n",
        "    print(f\"‚úì scipy {scipy.__version__} - OK\")\n",
        "    print(\"\\n‚úì Compatibility fixed! Now RESTART RUNTIME and continue.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Error: {e}\")\n",
        "    print(\"Please restart runtime and try again\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "print(\"Installing required packages...\")\n",
        "print(\"Fixing numpy/scipy compatibility (this is critical!)...\")\n",
        "print(\"‚ö† This may take a few minutes and show dependency warnings - that's OK\\n\")\n",
        "\n",
        "# STEP 1: Force uninstall numpy 2.x and install compatible numpy 1.x\n",
        "print(\"Step 1: Installing compatible numpy version...\")\n",
        "# Uninstall all numpy versions\n",
        "!pip uninstall -y numpy numpy-base || true\n",
        "# Also uninstall packages that force numpy 2.x (optional - will show errors if not installed)\n",
        "!pip uninstall -y pytensor shap jax jaxlib opencv-python-headless || true\n",
        "# Install numpy 1.26.4 (last stable 1.x version)\n",
        "!pip install -q \"numpy==1.26.4\" --no-cache-dir --force-reinstall\n",
        "\n",
        "# STEP 2: Install scipy (compatible with numpy 1.x)\n",
        "print(\"\\nStep 2: Installing scipy...\")\n",
        "!pip install -q \"scipy==1.11.4\" --no-cache-dir\n",
        "\n",
        "# STEP 3: Install opencv with compatible numpy version (ignore dependency warnings)\n",
        "print(\"\\nStep 3: Installing OpenCV (will show warnings - that's OK)...\")\n",
        "# Install older opencv version (4.8.1) that works with numpy 1.x\n",
        "# Using --no-deps to avoid numpy version conflicts\n",
        "# Note: Dependency warnings are OK - opencv will work with numpy 1.x despite the warnings\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_opencv(package_name, version):\n",
        "    \"\"\"Install opencv package ignoring dependency conflicts.\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package_name}=={version}\", \"--no-deps\"])\n",
        "        print(f\"  ‚úì Installed {package_name}\")\n",
        "    except:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", f\"{package_name}=={version}\"])\n",
        "            print(f\"  ‚úì Installed {package_name} (with dependencies)\")\n",
        "        except:\n",
        "            print(f\"  ‚ö† {package_name} installation had issues but may still work\")\n",
        "\n",
        "install_opencv(\"opencv-python\", \"4.8.1.78\")\n",
        "install_opencv(\"opencv-contrib-python\", \"4.8.1.78\")\n",
        "\n",
        "# STEP 4: Install other core libraries\n",
        "print(\"\\nStep 4: Installing core libraries...\")\n",
        "!pip install -q \"pillow==11.0.0\"\n",
        "!pip install -q pywavelets\n",
        "!pip install -q reedsolo\n",
        "!pip install -q cryptography\n",
        "\n",
        "# STEP 5: Install deep learning libraries\n",
        "print(\"\\nStep 5: Installing deep learning libraries...\")\n",
        "# Install PyTorch - will auto-detect CUDA if available\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "# STEP 6: Install utility libraries\n",
        "print(\"\\nStep 6: Installing utility libraries...\")\n",
        "!pip install -q tqdm\n",
        "!pip install -q pyyaml\n",
        "!pip install -q lpips\n",
        "!pip install -q scikit-image\n",
        "\n",
        "print(\"\\n‚úì All packages installed!\")\n",
        "\n",
        "# Verify numpy version\n",
        "try:\n",
        "    import numpy as np\n",
        "    np_version = np.__version__\n",
        "    major_version = int(np_version.split('.')[0])\n",
        "    print(f\"\\n‚úì Installed numpy version: {np_version}\")\n",
        "    \n",
        "    if major_version >= 2:\n",
        "        print(\"‚ö† WARNING: numpy 2.x detected! This will cause scipy errors.\")\n",
        "        print(\"  Trying to force reinstall numpy 1.26.4...\")\n",
        "        !pip uninstall -y numpy numpy-base\n",
        "        !pip install -q \"numpy==1.26.4\" --force-reinstall --no-cache-dir\n",
        "        # Reload numpy\n",
        "        import importlib\n",
        "        importlib.reload(sys.modules.get('numpy', None))\n",
        "        import numpy as np\n",
        "        np_version = np.__version__\n",
        "        major_version = int(np_version.split('.')[0])\n",
        "        print(f\"  After fix: numpy {np_version}\")\n",
        "    \n",
        "    if major_version < 2:\n",
        "        print(\"‚úì numpy version is compatible (< 2.0)\")\n",
        "    else:\n",
        "        print(\"‚ö† WARNING: Still numpy 2.x - RESTART RUNTIME and run this cell again\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Error checking numpy: {e}\")\n",
        "    print(\"  Please RESTART RUNTIME and run this cell again\")\n",
        "\n",
        "print(\"\\n‚úì Package installation complete!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CRITICAL: RESTART THE RUNTIME NOW!\")\n",
        "print(\"Runtime > Restart runtime\")\n",
        "print(\"Then run Cell 4 (Install Dependencies) again\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths and directories\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================\n",
        "# GOOGLE COLAB SETUP\n",
        "# ============================================\n",
        "# Mount Google Drive (uncomment if not already mounted)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    # Check if already mounted\n",
        "    if os.path.exists('/content/drive/MyDrive'):\n",
        "        print(\"‚úì Google Drive already mounted\")\n",
        "    else:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"‚úì Google Drive mounted\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö† Google Drive mount issue: {e}\")\n",
        "    print(\"If you get I/O errors, try remounting:\")\n",
        "    print(\"  from google.colab import drive\")\n",
        "    print(\"  drive.flush_and_unmount()\")\n",
        "    print(\"  drive.mount('/content/drive')\")\n",
        "\n",
        "# Set your project path here for Google Colab\n",
        "BASE_DIR = Path('/content/drive/MyDrive/project_codes/models_new')\n",
        "\n",
        "# ============================================\n",
        "# LOCAL EXECUTION (if not using Colab)\n",
        "# ============================================\n",
        "# Uncomment and modify if running locally:\n",
        "# BASE_DIR = Path.cwd()\n",
        "# # Or specify your local path:\n",
        "# # BASE_DIR = Path(r\"C:\\Users\\rehem\\Downloads\\other models\\others\\project2\")\n",
        "\n",
        "# Verify the path exists\n",
        "if not BASE_DIR.exists():\n",
        "    print(f\"‚ö† Warning: BASE_DIR does not exist: {BASE_DIR}\")\n",
        "    print(\"  Trying to find project root...\")\n",
        "    # Try to find project root by looking for key files\n",
        "    possible_paths = [\n",
        "        Path('/content/drive/MyDrive/project_codes/models_new'),\n",
        "        Path('/content/drive/MyDrive/project2'),\n",
        "        Path.cwd(),\n",
        "        Path.cwd().parent,\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if path.exists() and (path / \"models\").exists() and (path / \"training\").exists():\n",
        "            BASE_DIR = path\n",
        "            print(f\"  Found project at: {BASE_DIR}\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"  Could not find project root automatically\")\n",
        "        print(f\"  Current working directory: {Path.cwd()}\")\n",
        "        print(f\"  Please set BASE_DIR manually in this cell\")\n",
        "\n",
        "# Change to project directory (important for Colab)\n",
        "os.chdir(BASE_DIR)\n",
        "print(f\"‚úì Changed working directory to: {BASE_DIR}\")\n",
        "\n",
        "# Add project root to path (at the beginning to ensure it's found first)\n",
        "if str(BASE_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "# Verify the path is correct\n",
        "print(f\"\\nWorking directory: {BASE_DIR}\")\n",
        "print(f\"Python path includes: {BASE_DIR}\")\n",
        "\n",
        "# Verify key directories exist\n",
        "if (BASE_DIR / \"models\").exists() and (BASE_DIR / \"training\").exists():\n",
        "    print(\"‚úì Project structure verified\")\n",
        "    print(f\"  - models/ directory: ‚úì\")\n",
        "    print(f\"  - training/ directory: ‚úì\")\n",
        "else:\n",
        "    print(\"‚ö† Warning: Could not find 'models' or 'training' directories\")\n",
        "    print(f\"  Current directory: {BASE_DIR}\")\n",
        "    print(f\"  Contents: {list(BASE_DIR.iterdir())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "directories = [\n",
        "    BASE_DIR / \"dataset\" / \"originals\",\n",
        "    BASE_DIR / \"dataset\" / \"watermarked\",\n",
        "    BASE_DIR / \"dataset\" / \"tampered\",\n",
        "    BASE_DIR / \"dataset\" / \"metadata\",\n",
        "    BASE_DIR / \"outputs\" / \"checkpoints\",\n",
        "    BASE_DIR / \"outputs\" / \"logs\",\n",
        "    BASE_DIR / \"outputs\" / \"eval_reports\",\n",
        "    BASE_DIR / \"outputs\" / \"visual_results\",\n",
        "]\n",
        "\n",
        "for directory in directories:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"‚úì Created/verified: {directory}\")\n",
        "\n",
        "print(\"\\n‚úì All directories ready!\")\n",
        "\n",
        "# Verify imports work\n",
        "print(\"\\nVerifying imports and versions...\")\n",
        "\n",
        "# Check numpy/scipy versions first\n",
        "try:\n",
        "    import numpy as np\n",
        "    import scipy\n",
        "    print(f\"‚úì numpy {np.__version__} imported\")\n",
        "    print(f\"‚úì scipy {scipy.__version__} imported\")\n",
        "    \n",
        "    # Check if numpy version is compatible (should be < 2.0)\n",
        "    np_version = tuple(map(int, np.__version__.split('.')[:2]))\n",
        "    if np_version[0] >= 2:\n",
        "        print(f\"‚ö† WARNING: numpy {np.__version__} may cause compatibility issues\")\n",
        "        print(\"  Consider restarting runtime and reinstalling with: numpy<2.0\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö† numpy/scipy import error: {e}\")\n",
        "\n",
        "try:\n",
        "    # Test pywavelets import (the correct way)\n",
        "    import pywt\n",
        "    print(\"‚úì pywavelets (pywt) imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö† pywavelets import error: {e}\")\n",
        "    print(\"  Try restarting the runtime and running the installation cell again\")\n",
        "\n",
        "try:\n",
        "    import training.utils_data\n",
        "    import models.hybrid_multidomain_embed\n",
        "    print(\"‚úì Project modules imported successfully\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö† Import error: {e}\")\n",
        "    print(f\"  Make sure you're running from the project root directory\")\n",
        "    print(f\"  Current BASE_DIR: {BASE_DIR}\")\n",
        "    print(f\"  sys.path: {sys.path[:3]}\")  # Show first 3 paths\n",
        "    print(\"\\n  If you see numpy/scipy errors, restart runtime and run installation cell again\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Check Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if original images exist\n",
        "# Make sure BASE_DIR is in path (in case this cell is run independently)\n",
        "if str(BASE_DIR) not in sys.path:\n",
        "    sys.path.insert(0, str(BASE_DIR))\n",
        "\n",
        "try:\n",
        "    from training.utils_data import DatasetGenerator\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing: {e}\")\n",
        "    print(f\"BASE_DIR: {BASE_DIR}\")\n",
        "    print(f\"Checking if training directory exists: {(BASE_DIR / 'training').exists()}\")\n",
        "    print(f\"Contents of BASE_DIR: {list(BASE_DIR.iterdir())}\")\n",
        "    raise\n",
        "\n",
        "originals_dir = BASE_DIR / \"dataset\" / \"originals\"\n",
        "\n",
        "# First, check if directory exists and list ALL files\n",
        "print(f\"Checking directory: {originals_dir}\")\n",
        "print(f\"Directory exists: {originals_dir.exists()}\")\n",
        "\n",
        "# Try multiple methods to access files (Google Drive can be finicky)\n",
        "all_files = []\n",
        "image_files = []\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.JPG', '.JPEG', '.PNG', '.BMP', '.TIF', '.TIFF']\n",
        "\n",
        "if originals_dir.exists():\n",
        "    try:\n",
        "        # Method 1: Try pathlib iterdir\n",
        "        all_files = list(originals_dir.iterdir())\n",
        "        print(f\"\\n‚úì Successfully accessed directory using pathlib\")\n",
        "    except (OSError, IOError) as e:\n",
        "        print(f\"\\n‚ö† Pathlib access failed: {e}\")\n",
        "        print(\"Trying alternative method (os.listdir)...\")\n",
        "        \n",
        "        try:\n",
        "            # Method 2: Try os.listdir\n",
        "            import os\n",
        "            file_names = os.listdir(str(originals_dir))\n",
        "            all_files = [originals_dir / name for name in file_names]\n",
        "            print(f\"‚úì Successfully accessed directory using os.listdir\")\n",
        "        except (OSError, IOError) as e2:\n",
        "            print(f\"‚ö† os.listdir also failed: {e2}\")\n",
        "            print(\"\\nüîß Google Drive sync issue detected!\")\n",
        "            print(\"\\nSolutions:\")\n",
        "            print(\"  1. Unmount and remount Google Drive:\")\n",
        "            print(\"     - Run: drive.flush_and_unmount()\")\n",
        "            print(\"     - Then remount with: drive.mount('/content/drive')\")\n",
        "            print(\"  2. Check if files are actually in Google Drive\")\n",
        "            print(\"  3. Try accessing via Colab file browser (left sidebar)\")\n",
        "            print(f\"  4. Verify path exists: {originals_dir}\")\n",
        "            \n",
        "            # Try to create a test to see if we can write\n",
        "            try:\n",
        "                test_file = originals_dir / \".test_write\"\n",
        "                test_file.touch()\n",
        "                test_file.unlink()\n",
        "                print(\"  ‚úì Directory is writable\")\n",
        "            except Exception as e3:\n",
        "                print(f\"  ‚ö† Directory may not be accessible: {e3}\")\n",
        "            \n",
        "            # Set empty list so we can continue\n",
        "            all_files = []\n",
        "    \n",
        "    # Process files if we got any\n",
        "    if len(all_files) > 0:\n",
        "        print(f\"\\nTotal files/folders in directory: {len(all_files)}\")\n",
        "        \n",
        "        print(\"\\nAll files/folders found:\")\n",
        "        for f in all_files[:20]:  # Show first 20\n",
        "            try:\n",
        "                file_type = \"DIR\" if f.is_dir() else \"FILE\"\n",
        "                print(f\"  [{file_type}] {f.name} ({f.suffix if f.suffix else 'no extension'})\")\n",
        "            except:\n",
        "                print(f\"  [?] {f.name if hasattr(f, 'name') else str(f)}\")\n",
        "        if len(all_files) > 20:\n",
        "            print(f\"  ... and {len(all_files) - 20} more\")\n",
        "        \n",
        "        # Check for images with various extensions (case-insensitive)\n",
        "        for file in all_files:\n",
        "            try:\n",
        "                if file.is_file():\n",
        "                    ext = file.suffix\n",
        "                    if ext in image_extensions or ext.lower() in [e.lower() for e in image_extensions]:\n",
        "                        image_files.append(file)\n",
        "            except:\n",
        "                # Skip files that can't be accessed\n",
        "                continue\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Found {len(image_files)} images\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        if len(image_files) > 0:\n",
        "            print(f\"\\nImage files found:\")\n",
        "            for img in image_files[:10]:\n",
        "                print(f\"  ‚úì {img.name} ({img.suffix})\")\n",
        "            if len(image_files) > 10:\n",
        "                print(f\"  ... and {len(image_files) - 10} more images\")\n",
        "            print(\"\\n‚úì Ready to proceed with watermarking!\")\n",
        "        else:\n",
        "            print(\"\\n‚ö† No image files found with standard extensions\")\n",
        "            print(\"\\nTroubleshooting:\")\n",
        "            print(\"  1. Check if images are in subdirectories:\")\n",
        "            subdirs = [f for f in all_files if f.is_dir()]\n",
        "            if subdirs:\n",
        "                for subdir in subdirs[:5]:\n",
        "                    try:\n",
        "                        subdir_files = list(subdir.iterdir())\n",
        "                        print(f\"     - {subdir.name}/ has {len(subdir_files)} items\")\n",
        "                    except:\n",
        "                        print(f\"     - {subdir.name}/ (cannot access)\")\n",
        "            print(\"  2. Check file extensions - supported: .jpg, .jpeg, .png, .bmp, .tif, .tiff\")\n",
        "            print(\"  3. Make sure files are uploaded to Google Drive at:\")\n",
        "            print(f\"     {originals_dir}\")\n",
        "            print(\"  4. Try refreshing the file browser in Colab\")\n",
        "    else:\n",
        "        print(\"\\n‚ö† No files found in directory\")\n",
        "        print(\"This could mean:\")\n",
        "        print(\"  1. Directory is empty\")\n",
        "        print(\"  2. Google Drive sync issue (try remounting)\")\n",
        "        print(\"  3. Files are in a different location\")\n",
        "else:\n",
        "    print(f\"\\n‚ö† Directory does not exist: {originals_dir}\")\n",
        "    print(\"Creating directory...\")\n",
        "    try:\n",
        "        originals_dir.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"‚úì Created: {originals_dir}\")\n",
        "        print(\"\\nPlease upload your images to this directory and run this cell again.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Could not create directory: {e}\")\n",
        "        print(\"You may need to create it manually in Google Drive\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3.5: Split Dataset into Train/Validation/Test Sets\n",
        "\n",
        "**Important:** This step splits your images into training (70%), validation (20%), and testing (10%) sets to avoid data leakage and overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset into Train/Validation/Test sets\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "TOTAL_IMAGES_TO_USE = 18000  # Set to None to use all images, or specify a number\n",
        "TRAIN_PERCENT = 0.70\n",
        "VAL_PERCENT = 0.20\n",
        "TEST_PERCENT = 0.10\n",
        "\n",
        "# Verify percentages sum to 1.0\n",
        "assert abs(TRAIN_PERCENT + VAL_PERCENT + TEST_PERCENT - 1.0) < 0.001, \"Percentages must sum to 1.0\"\n",
        "\n",
        "# Source and destination directories\n",
        "originals_dir = BASE_DIR / \"dataset\" / \"originals\"\n",
        "train_dir = BASE_DIR / \"dataset\" / \"train\"\n",
        "val_dir = BASE_DIR / \"dataset\" / \"val\"\n",
        "test_dir = BASE_DIR / \"dataset\" / \"test\"\n",
        "\n",
        "# Create split directories\n",
        "for split_dir in [train_dir, val_dir, test_dir]:\n",
        "    split_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"‚úì Created/verified: {split_dir}\")\n",
        "\n",
        "# Get all image files\n",
        "print(f\"\\nCollecting images from: {originals_dir}\")\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.JPG', '.JPEG', '.PNG', '.BMP', '.TIF', '.TIFF']\n",
        "\n",
        "all_images = []\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(list(originals_dir.glob(f'*{ext}')))\n",
        "\n",
        "# Remove duplicates and sort for reproducibility\n",
        "all_images = sorted(list(set(all_images)))\n",
        "total_images = len(all_images)\n",
        "\n",
        "print(f\"Found {total_images} total images\")\n",
        "\n",
        "# Limit to specified number if requested\n",
        "if TOTAL_IMAGES_TO_USE and TOTAL_IMAGES_TO_USE < total_images:\n",
        "    print(f\"Limiting to {TOTAL_IMAGES_TO_USE} images (random selection)\")\n",
        "    random.seed(42)  # For reproducibility\n",
        "    all_images = random.sample(all_images, TOTAL_IMAGES_TO_USE)\n",
        "    total_images = len(all_images)\n",
        "\n",
        "# Calculate split sizes\n",
        "train_size = int(total_images * TRAIN_PERCENT)\n",
        "val_size = int(total_images * VAL_PERCENT)\n",
        "test_size = total_images - train_size - val_size  # Remaining goes to test\n",
        "\n",
        "print(f\"\\nDataset Split Configuration:\")\n",
        "print(f\"  Total images: {total_images}\")\n",
        "print(f\"  Training: {train_size} images ({TRAIN_PERCENT*100:.1f}%)\")\n",
        "print(f\"  Validation: {val_size} images ({VAL_PERCENT*100:.1f}%)\")\n",
        "print(f\"  Testing: {test_size} images ({TEST_PERCENT*100:.1f}%)\")\n",
        "\n",
        "# Shuffle images with fixed seed for reproducibility\n",
        "random.seed(42)\n",
        "shuffled_images = all_images.copy()\n",
        "random.shuffle(shuffled_images)\n",
        "\n",
        "# Split images\n",
        "train_images = shuffled_images[:train_size]\n",
        "val_images = shuffled_images[train_size:train_size + val_size]\n",
        "test_images = shuffled_images[train_size + val_size:]\n",
        "\n",
        "print(f\"\\nSplitting images...\")\n",
        "\n",
        "# Copy images to respective directories\n",
        "def copy_images(image_list, dest_dir, split_name):\n",
        "    \"\"\"Copy images to destination directory.\"\"\"\n",
        "    for img_path in image_list:\n",
        "        dest_path = dest_dir / img_path.name\n",
        "        try:\n",
        "            shutil.copy2(img_path, dest_path)\n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ö† Error copying {img_path.name}: {e}\")\n",
        "    print(f\"  ‚úì {split_name}: {len(image_list)} images copied to {dest_dir}\")\n",
        "\n",
        "copy_images(train_images, train_dir, \"Training\")\n",
        "copy_images(val_images, val_dir, \"Validation\")\n",
        "copy_images(test_images, test_dir, \"Testing\")\n",
        "\n",
        "# Save split information for reproducibility\n",
        "split_info = {\n",
        "    'total_images': total_images,\n",
        "    'train_size': train_size,\n",
        "    'val_size': val_size,\n",
        "    'test_size': test_size,\n",
        "    'train_percent': TRAIN_PERCENT,\n",
        "    'val_percent': VAL_PERCENT,\n",
        "    'test_percent': TEST_PERCENT,\n",
        "    'random_seed': 42,\n",
        "    'train_images': [str(img.name) for img in train_images],\n",
        "    'val_images': [str(img.name) for img in val_images],\n",
        "    'test_images': [str(img.name) for img in test_images]\n",
        "}\n",
        "\n",
        "split_info_path = BASE_DIR / \"dataset\" / \"split_info.json\"\n",
        "import json\n",
        "with open(split_info_path, 'w') as f:\n",
        "    json.dump(split_info, f, indent=2)\n",
        "\n",
        "print(f\"\\n‚úì Split information saved to: {split_info_path}\")\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Dataset Split Complete!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Training set: {train_dir} ({train_size} images)\")\n",
        "print(f\"Validation set: {val_dir} ({val_size} images)\")\n",
        "print(f\"Testing set: {test_dir} ({test_size} images)\")\n",
        "print(f\"\\nNote: Original images remain in {originals_dir}\")\n",
        "print(\"The splits are reproducible (seed=42) and saved in split_info.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Initialize Watermarking Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import watermarking models\n",
        "from models.hybrid_multidomain_embed import HybridMultiDomainEmbedder\n",
        "from models.hybrid_multidomain_verify import HybridMultiDomainVerifier\n",
        "from models.hybrid_multidomain_embed_det import HybridMultiDomainEmbedderDet\n",
        "from models.hybrid_multidomain_verify_det import HybridMultiDomainVerifierDet\n",
        "\n",
        "# PATCH: Fix the bytearray conversion issue - COMPREHENSIVE FIX\n",
        "# This fixes the \"sequence item 0: expected a bytes-like object, int found\" error\n",
        "import types\n",
        "import struct\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "from cryptography.hazmat.primitives.asymmetric import padding\n",
        "\n",
        "def _patched_create_deterministic_payload(self, encoded_payload):\n",
        "    \"\"\"Patched version - handles bytearray conversion correctly.\"\"\"\n",
        "    # Convert encoded_payload to bytes for RSA signing\n",
        "    # encoded_payload is bytearray from prepare_payload\n",
        "    if isinstance(encoded_payload, bytearray):\n",
        "        payload_bytes = bytes(encoded_payload)  # This conversion always works\n",
        "    elif isinstance(encoded_payload, bytes):\n",
        "        payload_bytes = encoded_payload\n",
        "    else:\n",
        "        # Convert iterable to bytes via list comprehension\n",
        "        payload_bytes = bytes([int(b) & 0xFF for b in encoded_payload])\n",
        "    \n",
        "    # Sign the payload\n",
        "    signature = self.private_key.sign(\n",
        "        payload_bytes,\n",
        "        padding.PSS(\n",
        "            mgf=padding.MGF1(hashes.SHA256()),\n",
        "            salt_length=padding.PSS.MAX_LENGTH\n",
        "        ),\n",
        "        hashes.SHA256()\n",
        "    )\n",
        "    \n",
        "    # Create headers\n",
        "    payload_len = len(encoded_payload)\n",
        "    sig_len = len(signature)\n",
        "    header = struct.pack('>I', payload_len)  # Returns bytes\n",
        "    sig_header = struct.pack('>I', sig_len)  # Returns bytes\n",
        "    \n",
        "    # Concatenate: convert all to bytearray for compatibility\n",
        "    # header and sig_header are bytes, signature is bytes, encoded_payload is bytearray\n",
        "    # CRITICAL: Don't wrap bytearray in bytearray() - that causes the error!\n",
        "    header_ba = bytearray(header)  # bytes -> bytearray\n",
        "    payload_ba = encoded_payload if isinstance(encoded_payload, bytearray) else bytearray(encoded_payload)\n",
        "    sig_header_ba = bytearray(sig_header)  # bytes -> bytearray\n",
        "    signature_ba = bytearray(signature)  # bytes -> bytearray\n",
        "    \n",
        "    final_payload = header_ba + payload_ba + sig_header_ba + signature_ba\n",
        "    \n",
        "    return final_payload, signature\n",
        "\n",
        "def _patched_prepare_payload(self, message: str):\n",
        "    \"\"\"Patched version - handles reedsolo output correctly.\"\"\"\n",
        "    from typing import Tuple, Dict\n",
        "    \n",
        "    # Convert message to bytes\n",
        "    message_bytes = message.encode('utf-8')\n",
        "    \n",
        "    # Encode with Reed-Solomon ECC\n",
        "    # reedsolo.encode() returns bytearray - use it directly\n",
        "    rs_encoded = self.rs_codec.encode(message_bytes)\n",
        "    \n",
        "    # CRITICAL FIX: Use bytearray directly, don't wrap it\n",
        "    # The error happens when you do bytearray(bytearray(...))\n",
        "    if isinstance(rs_encoded, bytearray):\n",
        "        encoded_payload = rs_encoded  # Use directly - don't wrap!\n",
        "    elif isinstance(rs_encoded, bytes):\n",
        "        encoded_payload = bytearray(rs_encoded)  # Convert bytes to bytearray\n",
        "    else:\n",
        "        # Fallback: build bytearray element by element\n",
        "        encoded_payload = bytearray()\n",
        "        for item in rs_encoded:\n",
        "            if isinstance(item, int):\n",
        "                encoded_payload.append(item & 0xFF)\n",
        "            else:\n",
        "                encoded_payload.append(int(item) & 0xFF)\n",
        "    \n",
        "    # Create deterministic payload with signature\n",
        "    final_payload, signature = self.create_deterministic_payload(encoded_payload)\n",
        "    \n",
        "    # Create metadata\n",
        "    metadata = {\n",
        "        'original_length': len(message_bytes),\n",
        "        'encoded_length': len(encoded_payload),\n",
        "        'final_length': len(final_payload),\n",
        "        'signature_length': len(signature),\n",
        "        'ecc_symbols': self.ecc_symbols,\n",
        "        'header_structure': {\n",
        "            'payload_len_bytes': 4,\n",
        "            'sig_len_bytes': 4\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return final_payload, metadata\n",
        "\n",
        "# PATCH: Fix the save_public_key method - cryptography API fix\n",
        "def _patched_save_public_key(self, path: str):\n",
        "    \"\"\"Patched version - uses correct cryptography API method.\"\"\"\n",
        "    from cryptography.hazmat.primitives import serialization\n",
        "    \n",
        "    # Correct method: public_bytes() not public_key_bytes()\n",
        "    pem = self.public_key.public_bytes(\n",
        "        encoding=serialization.Encoding.PEM,\n",
        "        format=serialization.PublicFormat.SubjectPublicKeyInfo\n",
        "    )\n",
        "    with open(path, 'wb') as f:\n",
        "        f.write(pem)\n",
        "\n",
        "# Apply patches to fix the bytearray conversion bug and cryptography API\n",
        "HybridMultiDomainEmbedderDet.create_deterministic_payload = _patched_create_deterministic_payload\n",
        "HybridMultiDomainEmbedderDet.prepare_payload = _patched_prepare_payload\n",
        "HybridMultiDomainEmbedderDet.save_public_key = _patched_save_public_key\n",
        "print(\"‚úì Applied comprehensive patches to fix bytearray conversion and cryptography API issues\")\n",
        "\n",
        "# Initialize embedder and verifier\n",
        "# ULTRA-IMPERCEPTIBLE WATERMARKING: Optimized for maximum invisibility\n",
        "# - Ultra-low alpha values (0.8-1.0) for minimal visual impact\n",
        "# - Adaptive embedding adjusts strength based on local image characteristics\n",
        "# - Optimized perceptual masking (0.3-1.5x) embeds conservatively\n",
        "# - Minimal modification algorithms reduce changes to absolute minimum\n",
        "# - Images should be NEARLY IDENTICAL to originals!\n",
        "\n",
        "# Phase 1: Basic embedding/verification\n",
        "embedder = HybridMultiDomainEmbedder(\n",
        "    block_size_dct=8,\n",
        "    block_size_dwt=16,\n",
        "    ecc_symbols=20,\n",
        "    alpha_dct=0.05,      # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    alpha_dwt=0.05,      # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    alpha_svd=0.1,     # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    redundancy=1,  # Reduced from 3 to minimize distortions\n",
        "    use_adaptive=True,           # Enable adaptive embedding\n",
        "    use_perceptual_masking=True  # Enable optimized perceptual masking\n",
        ")\n",
        "\n",
        "verifier = HybridMultiDomainVerifier(\n",
        "    block_size_dct=8,\n",
        "    block_size_dwt=16,\n",
        "    ecc_symbols=20\n",
        ")\n",
        "\n",
        "# Phase 2: With tamper detection (using ultra-extreme-imperceptible settings)\n",
        "embedder_det = HybridMultiDomainEmbedderDet(\n",
        "    block_size_dct=8,\n",
        "    block_size_dwt=16,\n",
        "    ecc_symbols=20,\n",
        "    alpha_dct=0.05,      # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    alpha_dwt=0.05,      # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    alpha_svd=0.1,     # ULTRA-EXTREME low for maximum imperceptibility\n",
        "    redundancy=1,  # Reduced from 3 to minimize distortions\n",
        "    use_adaptive=True,           # Enable adaptive embedding\n",
        "    use_perceptual_masking=True  # Enable optimized perceptual masking\n",
        ")\n",
        "\n",
        "verifier_det = HybridMultiDomainVerifierDet(\n",
        "    block_size_dct=8,\n",
        "    block_size_dwt=16,\n",
        "    ecc_symbols=20\n",
        ")\n",
        "\n",
        "print(\"‚úì Models initialized successfully\")\n",
        "print(\"  Using MAXIMUM-IMPERCEPTIBLE watermarking settings:\")\n",
        "print(\"    - Color space: BGR (OpenCV standard, preserves RGB color)\")\n",
        "print(\"    - Embedding domain: DCT ONLY (most imperceptible, avoids distortion accumulation)\")\n",
        "print(\"    - Alpha DCT: 0.05 (99.4% reduction from original)\")\n",
        "print(\"    - Modification factor: 0.1x alpha (90% reduction in actual modifications)\")\n",
        "print(\"    - Redundancy: 1 (minimized to reduce distortions)\")\n",
        "print(\"    - Color preservation: Enabled (processes BGR channels separately)\")\n",
        "print(\"    - Adaptive embedding: Enabled (optimized)\")\n",
        "print(\"    - Perceptual masking: Enabled (conservative 0.3-1.5x)\")\n",
        "print(\"    - Single clipping: No double-clipping artifacts\")\n",
        "print(\"  ‚≠ê Watermarked images should be VISUALLY IDENTICAL to originals! ‚≠ê\")\n",
        "print(\"  ‚ö†Ô∏è  Note: Using DCT-only embedding for maximum imperceptibility.\")\n",
        "print(\"     Multi-domain embedding (DCT+DWT+SVD) is disabled to prevent distortion accumulation.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Watermarked Dataset\n",
        "\n",
        "**Now processing each split (Train/Val/Test) separately to maintain data separation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Choose which splits to process\n",
        "PROCESS_TRAIN = True   # Process training set\n",
        "PROCESS_VAL = True     # Process validation set  \n",
        "PROCESS_TEST = True    # Process test set\n",
        "# Set to False to skip a split (useful for testing with smaller datasets first)\n",
        "\n",
        "message = \"Copyright: Dataset 2024\"\n",
        "print(f\"Generating watermarked dataset with message: '{message}'\")\n",
        "print(\"\\nProcessing splits separately to maintain data separation:\")\n",
        "\n",
        "# Control which splits to process (set to True to process that split)\n",
        "PROCESS_TRAIN = True   # Process training set\n",
        "PROCESS_VAL = True     # Process validation set\n",
        "PROCESS_TEST = True    # Process test set\n",
        "\n",
        "all_metadata = {}\n",
        "\n",
        "# Process Training Set\n",
        "if PROCESS_TRAIN:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Processing TRAINING set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    train_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"train\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"train\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    train_metadata = train_gen.generate_watermarked_dataset(\n",
        "        message=message,\n",
        "        metadata_file=\"train_metadata.json\"\n",
        "    )\n",
        "    all_metadata['train'] = train_metadata\n",
        "    print(f\"‚úì Training: {len(train_metadata)} watermarked images\")\n",
        "\n",
        "# Process Validation Set\n",
        "if PROCESS_VAL:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Processing VALIDATION set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    val_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"val\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"val\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    val_metadata = val_gen.generate_watermarked_dataset(\n",
        "        message=message,\n",
        "        metadata_file=\"val_metadata.json\"\n",
        "    )\n",
        "    all_metadata['val'] = val_metadata\n",
        "    print(f\"‚úì Validation: {len(val_metadata)} watermarked images\")\n",
        "\n",
        "# Process Test Set\n",
        "if PROCESS_TEST:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Processing TEST set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    test_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"test\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"test\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    test_metadata = test_gen.generate_watermarked_dataset(\n",
        "        message=message,\n",
        "        metadata_file=\"test_metadata.json\"\n",
        "    )\n",
        "    all_metadata['test'] = test_metadata\n",
        "    print(f\"‚úì Test: {len(test_metadata)} watermarked images\")\n",
        "\n",
        "# Summary\n",
        "total_watermarked = sum(len(meta) for meta in all_metadata.values())\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"SUMMARY: Generated {total_watermarked} total watermarked images\")\n",
        "print(f\"{'='*60}\")\n",
        "for split_name, metadata in all_metadata.items():\n",
        "    print(f\"  {split_name.upper()}: {len(metadata)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Generate Tampered/Attacked Dataset\n",
        "\n",
        "This will generate attacked versions for each split (train/val/test) separately to maintain data separation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define attacks to apply\n",
        "from training.attacks import AttackSimulator\n",
        "import numpy as np\n",
        "\n",
        "# Configure attacks\n",
        "attacks = [\n",
        "    ('jpeg_compression', {'quality': 75}),\n",
        "    ('jpeg_compression', {'quality': 50}),\n",
        "    ('crop', {'crop_percent': 0.9}),\n",
        "    ('crop_corner', {'crop_percent': 0.1}),\n",
        "    ('remove_region', {'x': 100, 'y': 100, 'width': 150, 'height': 150}),\n",
        "    ('blur', {'kernel_size': 5}),\n",
        "    ('blur', {'kernel_size': 7}),\n",
        "    ('noise', {'noise_level': 0.02}),\n",
        "    ('noise', {'noise_level': 0.05}),\n",
        "    ('rotate', {'angle': 5}),\n",
        "    ('rotate', {'angle': -10}),\n",
        "    ('resize', {'scale': 0.8}),\n",
        "    ('add_text', {'text': 'TAMPERED', 'position': (50, 50)}),\n",
        "    ('add_text', {'text': 'WATERMARK', 'position': (100, 100)}),\n",
        "    ('add_noise_patch', {'patch_size': (50, 50)}),\n",
        "    ('draw_rectangle', {'pt1': (50, 50), 'pt2': (200, 200)}),\n",
        "    ('draw_circle', {'center': (150, 150), 'radius': 50}),\n",
        "    ('brightness', {'factor': 1.2}),\n",
        "    ('contrast', {'factor': 1.3}),\n",
        "]\n",
        "\n",
        "print(f\"Applying {len(attacks)} different attack types...\")\n",
        "print(\"Each watermarked image will get 3 random attacks applied.\")\n",
        "print(\"\\nProcessing each split separately to maintain data separation...\")\n",
        "\n",
        "# Control which splits to process (set to True to process that split)\n",
        "PROCESS_TRAIN = True   # Process training set\n",
        "PROCESS_VAL = True     # Process validation set\n",
        "PROCESS_TEST = True    # Process test set\n",
        "\n",
        "all_tampered_metadata = {}\n",
        "\n",
        "# Process Training Set\n",
        "if PROCESS_TRAIN:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Generating tampered images for TRAINING set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    train_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"train\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"train\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    train_tampered = train_gen.generate_tampered_dataset(\n",
        "        attacks=attacks,\n",
        "        num_attacks_per_image=3\n",
        "    )\n",
        "    all_tampered_metadata['train'] = train_tampered\n",
        "    print(f\"‚úì Training: Generated tampered images\")\n",
        "\n",
        "# Process Validation Set\n",
        "if PROCESS_VAL:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Generating tampered images for VALIDATION set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    val_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"val\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"val\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    val_tampered = val_gen.generate_tampered_dataset(\n",
        "        attacks=attacks,\n",
        "        num_attacks_per_image=3\n",
        "    )\n",
        "    all_tampered_metadata['val'] = val_tampered\n",
        "    print(f\"‚úì Validation: Generated tampered images\")\n",
        "\n",
        "# Process Test Set\n",
        "if PROCESS_TEST:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"Generating tampered images for TEST set...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    test_gen = DatasetGenerator(\n",
        "        originals_dir=str(BASE_DIR / \"dataset\" / \"test\"),\n",
        "        output_dir=str(BASE_DIR / \"dataset\" / \"test\"),\n",
        "        embedder=embedder_det,\n",
        "        verifier=verifier_det\n",
        "    )\n",
        "    test_tampered = test_gen.generate_tampered_dataset(\n",
        "        attacks=attacks,\n",
        "        num_attacks_per_image=3\n",
        "    )\n",
        "    all_tampered_metadata['test'] = test_tampered\n",
        "    print(f\"‚úì Test: Generated tampered images\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úì Tampered dataset generation complete for all splits!\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"Data separation maintained - no leakage between train/val/test sets\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verify Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify watermarked and tampered images\n",
        "print(\"Running verification on watermarked and tampered images...\")\n",
        "\n",
        "verification_results = dataset_gen.verify_dataset(verify_tampered=True)\n",
        "\n",
        "# Print summary\n",
        "wm_results = verification_results['watermarked']\n",
        "tampered_results = verification_results['tampered']\n",
        "\n",
        "wm_success = sum(1 for r in wm_results.values() if r.get('decode_success', False))\n",
        "tamper_detected = sum(1 for r in tampered_results.values() \n",
        "                     if not r.get('decode_success', True) or not r.get('hash_valid', True))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"VERIFICATION SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Watermarked images verified: {wm_success}/{len(wm_results)}\")\n",
        "print(f\"Tampered images detected: {tamper_detected}/{len(tampered_results)}\")\n",
        "print(f\"\\nResults saved to: {BASE_DIR / 'dataset' / 'verification_results.json'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Run Test Harness (Robustness Testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive test harness\n",
        "from training.test_harness_md_det import TestHarness\n",
        "import json\n",
        "\n",
        "# Initialize test harness\n",
        "test_harness = TestHarness(\n",
        "    embedder=embedder_det,\n",
        "    verifier=verifier_det,\n",
        "    output_dir=str(BASE_DIR / \"outputs\" / \"eval_reports\")\n",
        ")\n",
        "\n",
        "# Run tests on watermarked images\n",
        "print(\"Running robustness tests...\")\n",
        "print(\"This will test various attacks and measure robustness metrics.\")\n",
        "\n",
        "# Get a sample of watermarked images for testing\n",
        "watermarked_dir = BASE_DIR / \"dataset\" / \"watermarked\"\n",
        "test_images = list(watermarked_dir.glob(\"*.png\"))[:5]  # Test on first 5 images\n",
        "\n",
        "if len(test_images) > 0:\n",
        "    test_results = test_harness.run_tests(\n",
        "        image_paths=[str(img) for img in test_images],\n",
        "        message=message,\n",
        "        num_attacks_per_image=5\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n‚úì Test harness complete\")\n",
        "    print(f\"Results saved to: {BASE_DIR / 'outputs' / 'eval_reports'}\")\n",
        "else:\n",
        "    print(\"‚ö† No watermarked images found for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Train ML-Based Watermarking Models (Phase 3 - Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if ML training is enabled\n",
        "TRAIN_ML_MODELS = False  # Set to True to train ML models\n",
        "\n",
        "if not TRAIN_ML_MODELS:\n",
        "    print(\"ML model training is disabled. Set TRAIN_ML_MODELS = True to enable.\")\n",
        "    print(\"Note: ML training requires significant computational resources and time.\")\n",
        "else:\n",
        "    import torch\n",
        "    from torch.utils.data import DataLoader\n",
        "    from training.train_ml_watermark import MLWatermarkTrainer, WatermarkDataset\n",
        "    from models.ml_watermark_models import Encoder, Decoder, Discriminator\n",
        "    \n",
        "    # Check device\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Hyperparameters\n",
        "    message_bits = 100\n",
        "    ecc_symbols = 20\n",
        "    image_size = (256, 256)\n",
        "    batch_size = 8\n",
        "    num_epochs = 100\n",
        "    \n",
        "    # Create models\n",
        "    encoder = Encoder(message_length=message_bits, image_channels=1)\n",
        "    decoder = Decoder(message_length=message_bits, image_channels=1)\n",
        "    discriminator = Discriminator(image_channels=1)\n",
        "    \n",
        "    # Create trainer\n",
        "    trainer = MLWatermarkTrainer(\n",
        "        encoder, decoder, discriminator,\n",
        "        device=device,\n",
        "        message_bits=message_bits,\n",
        "        ecc_symbols=ecc_symbols,\n",
        "        use_discriminator=True\n",
        "    )\n",
        "    \n",
        "    # Create dataset from training split (maintains data separation)\n",
        "    train_dataset = WatermarkDataset(\n",
        "        image_dir=str(BASE_DIR / \"dataset\" / \"train\"),\n",
        "        image_size=image_size\n",
        "    )\n",
        "    \n",
        "    # Optional: Create validation dataset for monitoring during training\n",
        "    val_dataset = WatermarkDataset(\n",
        "        image_dir=str(BASE_DIR / \"dataset\" / \"val\"),\n",
        "        image_size=image_size\n",
        "    ) if (BASE_DIR / \"dataset\" / \"val\").exists() else None\n",
        "    \n",
        "    if len(train_dataset) == 0:\n",
        "        print(\"‚ö† No images found for training. Please add images to dataset/originals/\")\n",
        "    else:\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2 if device == 'cuda' else 0\n",
        "        )\n",
        "        \n",
        "        print(f\"Training on {len(train_dataset)} images\")\n",
        "        print(f\"Batch size: {batch_size}, Epochs: {num_epochs}\")\n",
        "        \n",
        "        # Train\n",
        "        checkpoint_dir = BASE_DIR / \"outputs\" / \"checkpoints\"\n",
        "        history = trainer.train(\n",
        "            train_loader,\n",
        "            num_epochs=num_epochs,\n",
        "            save_dir=str(checkpoint_dir),\n",
        "            save_interval=10\n",
        "        )\n",
        "        \n",
        "        print(\"\\n‚úì ML model training complete!\")\n",
        "        print(f\"Checkpoints saved to: {checkpoint_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Collect statistics\n",
        "from training.utils_data import DatasetLoader\n",
        "\n",
        "dataset_loader = DatasetLoader(str(BASE_DIR / \"dataset\"))\n",
        "stats = dataset_loader.get_statistics()\n",
        "\n",
        "# Load verification results if available\n",
        "verification_file = BASE_DIR / \"dataset\" / \"verification_results.json\"\n",
        "verification_data = {}\n",
        "if verification_file.exists():\n",
        "    with open(verification_file, 'r') as f:\n",
        "        verification_data = json.load(f)\n",
        "\n",
        "# Create summary report\n",
        "report = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'dataset_statistics': stats,\n",
        "    'verification_summary': {\n",
        "        'watermarked_total': len(verification_data.get('watermarked', {})),\n",
        "        'watermarked_success': sum(1 for r in verification_data.get('watermarked', {}).values() \n",
        "                                  if r.get('decode_success', False)),\n",
        "        'tampered_total': len(verification_data.get('tampered', {})),\n",
        "        'tampered_detected': sum(1 for r in verification_data.get('tampered', {}).values() \n",
        "                               if not r.get('decode_success', True) or not r.get('hash_valid', True)),\n",
        "    },\n",
        "    'directories': {\n",
        "        'originals': str(BASE_DIR / \"dataset\" / \"originals\"),\n",
        "        'watermarked': str(BASE_DIR / \"dataset\" / \"watermarked\"),\n",
        "        'tampered': str(BASE_DIR / \"dataset\" / \"tampered\"),\n",
        "        'outputs': str(BASE_DIR / \"outputs\"),\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save report\n",
        "report_file = BASE_DIR / \"outputs\" / \"summary_report.json\"\n",
        "with open(report_file, 'w') as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "# Print summary\n",
        "print(\"=\"*60)\n",
        "print(\"SUMMARY REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"  Original images: {stats['originals']}\")\n",
        "print(f\"  Watermarked images: {stats['watermarked']}\")\n",
        "print(f\"  Tampered images: {stats['tampered']}\")\n",
        "\n",
        "if verification_data:\n",
        "    print(f\"\\nVerification Results:\")\n",
        "    print(f\"  Watermarked success rate: {report['verification_summary']['watermarked_success']}/{report['verification_summary']['watermarked_total']}\")\n",
        "    print(f\"  Tampered detection rate: {report['verification_summary']['tampered_detected']}/{report['verification_summary']['tampered_total']}\")\n",
        "\n",
        "print(f\"\\nReport saved to: {report_file}\")\n",
        "print(\"\\n‚úì All processing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- **Folder Structure**: The folders (watermarked, tampered, output, etc.) should be ready and can be empty initially. The code will create them if they don't exist and populate them with data.\n",
        "- **Original Images**: Place your images in `dataset/originals/` before running Step 4.\n",
        "- **ML Training**: Phase 3 ML model training is optional and can be enabled by setting `TRAIN_ML_MODELS = True` in Step 8.\n",
        "- **Results**: All results are saved in the `outputs/` directory, including checkpoints, logs, and evaluation reports.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
