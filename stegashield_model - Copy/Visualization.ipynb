{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# StegaShield Visualization Dashboard\n",
        "\n",
        "This notebook inspects pairs of original vs. watermarked images, computes PSNR/SSIM, and produces summary plots similar to the reference screenshots.\n",
        "\n",
        "**Quick start**\n",
        "1. Set the dataset paths in the config cell below (expects `split/originals` and `split/watermarked`).\n",
        "2. Optional: run the `pip install` cell if you're in a fresh Colab/runtime.\n",
        "3. Execute the notebook top to bottom to regenerate tables and visualizations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies when running in Colab / fresh env\n",
        "# !pip install --quiet numpy pandas matplotlib seaborn pillow opencv-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import List, Dict\n",
        "import shutil\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display\n",
        "\n",
        "from stegashield_profiles import embed_image\n",
        "from training.test_harness_det import TestHarness\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "\n",
        "# --- Configuration ---\n",
        "ORIGINAL_SUBDIR = \"originals\"\n",
        "WATERMARKED_SUBDIR = \"watermarked\"\n",
        "MAX_PAIRS_PER_SPLIT = None  # set to an int to subsample\n",
        "DISPLAY_SAMPLE_COUNT = 3\n",
        "\n",
        "# Scenario 1: you already have dataset/<split>/{originals,watermarked}\n",
        "DATA_ROOT = Path(\"dataset\")\n",
        "SPLITS: List[str] = [\"train\", \"val\", \"test\"]\n",
        "\n",
        "# Scenario 2: you only have a folder of raw originals\n",
        "AUTO_BUILD_FROM_ORIGINALS = True\n",
        "ORIGINALS_ONLY_DIR = Path(\"originals_unused\")\n",
        "AUTO_DATA_ROOT = Path(\"dataset_autogen\")\n",
        "AUTO_SPLIT_NAME = \"auto\"\n",
        "WATERMARK_MODE = \"hybrid\"\n",
        "AUTO_MESSAGE = \"AutoProfile\"\n",
        "AUTO_USER_KEY = None  # set to a string if you want hashed payloads\n",
        "AUTO_MAX_ORIGINALS = 200  # set to None for no limit\n",
        "\n",
        "if AUTO_BUILD_FROM_ORIGINALS and ORIGINALS_ONLY_DIR.exists():\n",
        "    auto_orig_dir = AUTO_DATA_ROOT / AUTO_SPLIT_NAME / ORIGINAL_SUBDIR\n",
        "    auto_wm_dir = AUTO_DATA_ROOT / AUTO_SPLIT_NAME / WATERMARKED_SUBDIR\n",
        "    auto_orig_dir.mkdir(parents=True, exist_ok=True)\n",
        "    auto_wm_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    selected_paths = [p for p in ORIGINALS_ONLY_DIR.iterdir() if p.is_file()]\n",
        "    if AUTO_MAX_ORIGINALS is not None:\n",
        "        selected_paths = selected_paths[:AUTO_MAX_ORIGINALS]\n",
        "\n",
        "    for src_path in selected_paths:\n",
        "        dst_orig = auto_orig_dir / src_path.name\n",
        "        if not dst_orig.exists():\n",
        "            shutil.copy2(src_path, dst_orig)\n",
        "        dst_wm = auto_wm_dir / src_path.with_suffix(\".png\").name\n",
        "        if not dst_wm.exists():\n",
        "            try:\n",
        "                meta = embed_image(\n",
        "                    image_path=str(src_path),\n",
        "                    message=AUTO_MESSAGE,\n",
        "                    mode=WATERMARK_MODE,\n",
        "                    user_key=AUTO_USER_KEY,\n",
        "                    output_dir=str(auto_wm_dir),\n",
        "                )\n",
        "            except ValueError as exc:\n",
        "                print(f\"⚠️ Skipping {src_path.name}: {exc}\")\n",
        "                continue\n",
        "            produced_path = Path(meta[\"image_path\"])\n",
        "            if produced_path != dst_wm:\n",
        "                if dst_wm.exists():\n",
        "                    dst_wm.unlink()\n",
        "                shutil.move(produced_path, dst_wm)\n",
        "            produced_meta = Path(meta[\"metadata_path\"])\n",
        "            desired_meta = auto_wm_dir / f\"{dst_wm.stem}_metadata.json\"\n",
        "            if produced_meta != desired_meta:\n",
        "                if desired_meta.exists():\n",
        "                    desired_meta.unlink()\n",
        "                shutil.move(produced_meta, desired_meta)\n",
        "            print(f\"✅ Watermarked {src_path.name} → {dst_wm.name}\")\n",
        "        else:\n",
        "            print(f\"ℹ️ Already watermarked: {dst_wm.name}\")\n",
        "    DATA_ROOT = AUTO_DATA_ROOT\n",
        "    SPLITS = [AUTO_SPLIT_NAME]\n",
        "    print(f\"✓ Auto-generated dataset with {len(list(auto_orig_dir.iterdir()))} originals at {AUTO_DATA_ROOT}\")\n",
        "\n",
        "assert DATA_ROOT.exists(), f\"Dataset root not found: {DATA_ROOT.resolve()}\"\n",
        "\n",
        "def find_pairs(split: str) -> List[Dict]:\n",
        "    orig_dir = DATA_ROOT / split / ORIGINAL_SUBDIR\n",
        "    wm_dir = DATA_ROOT / split / WATERMARKED_SUBDIR\n",
        "    if not orig_dir.exists() or not wm_dir.exists():\n",
        "        print(f\"⚠️ Skipping split '{split}' (missing {orig_dir} or {wm_dir})\")\n",
        "        return []\n",
        "\n",
        "    orig_files = {p.stem: p for p in orig_dir.iterdir() if p.is_file()}\n",
        "    wm_files = {p.stem: p for p in wm_dir.iterdir() if p.is_file()}\n",
        "    common = sorted(set(orig_files.keys()) & set(wm_files.keys()))\n",
        "    if MAX_PAIRS_PER_SPLIT:\n",
        "        common = common[:MAX_PAIRS_PER_SPLIT]\n",
        "\n",
        "    pairs = []\n",
        "    for stem in common:\n",
        "        pairs.append({\n",
        "            \"split\": split,\n",
        "            \"image_id\": stem,\n",
        "            \"original_path\": orig_files[stem],\n",
        "            \"watermarked_path\": wm_files[stem],\n",
        "        })\n",
        "    print(f\"✓ {split}: found {len(pairs)} matched pairs\")\n",
        "    return pairs\n",
        "\n",
        "all_pairs: List[Dict] = []\n",
        "for split in SPLITS:\n",
        "    all_pairs.extend(find_pairs(split))\n",
        "\n",
        "if not all_pairs:\n",
        "    raise SystemExit(\"No overlapping images found between original/watermarked folders.\")\n",
        "\n",
        "len(all_pairs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Tip:** If you only have raw originals (e.g., `originals_unused/`), leave `AUTO_BUILD_FROM_ORIGINALS = True`. The notebook will copy them into `dataset_autogen/auto/originals`, embed hybrid watermarks on the fly, and use those generated outputs for every visualization cell below. Set the flag to `False` once you already have a curated dataset to avoid re-watermarking on each run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Run Attack Harness from Here\n",
        "Toggle `RUN_HARNESS = True` in the next cell to embed + attack-test a subset of your dataset without leaving this notebook. The harness will populate `outputs/results.csv`, which feeds the attack visualizations later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_HARNESS = False\n",
        "HARNESS_IMAGE_LIMIT = 50\n",
        "HARNESS_OUTPUT_DIR = Path(\"outputs\")\n",
        "HARNESS_RESULTS_NAME = \"results.csv\"\n",
        "HARNESS_MESSAGE_LSB = \"StegaShield_Dataset2025\"\n",
        "HARNESS_MESSAGE_SEMI = \"StegaShield_SemiFragile\"\n",
        "\n",
        "if RUN_HARNESS:\n",
        "    harness_images = []\n",
        "    for split in SPLITS:\n",
        "        src_dir = DATA_ROOT / split / ORIGINAL_SUBDIR\n",
        "        if not src_dir.exists():\n",
        "            continue\n",
        "        imgs = sorted([p for p in src_dir.iterdir() if p.is_file()])\n",
        "        if HARNESS_IMAGE_LIMIT:\n",
        "            imgs = imgs[:HARNESS_IMAGE_LIMIT]\n",
        "        harness_images.extend([str(p) for p in imgs])\n",
        "\n",
        "    if harness_images:\n",
        "        harness = TestHarness(output_dir=str(HARNESS_OUTPUT_DIR), lsb_message=HARNESS_MESSAGE_LSB, semi_message=HARNESS_MESSAGE_SEMI)\n",
        "        csv_path = harness.run_batch(harness_images, csv_name=HARNESS_RESULTS_NAME)\n",
        "        print(f\"✅ Test harness completed. Results stored at {csv_path}\")\n",
        "    else:\n",
        "        print(\"⚠️ No images found to run the harness. Check DATA_ROOT and splits.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "HARNESS_RESULTS_CSV = Path(\"outputs\") / \"results.csv\"\n",
        "print(f\"Harness CSV: {HARNESS_RESULTS_CSV}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_rgb(path: Path) -> np.ndarray:\n",
        "    arr = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
        "    if arr is None:\n",
        "        raise FileNotFoundError(path)\n",
        "    return cv2.cvtColor(arr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def compute_psnr(img_a: np.ndarray, img_b: np.ndarray) -> float:\n",
        "    return cv2.PSNR(img_a, img_b)\n",
        "\n",
        "\n",
        "def compute_ssim(img_a: np.ndarray, img_b: np.ndarray) -> float:\n",
        "    img_a_gray = cv2.cvtColor(img_a, cv2.COLOR_RGB2GRAY)\n",
        "    img_b_gray = cv2.cvtColor(img_b, cv2.COLOR_RGB2GRAY)\n",
        "    C1 = (0.01 * 255) ** 2\n",
        "    C2 = (0.03 * 255) ** 2\n",
        "    kernel = (11, 11)\n",
        "    sigma = 1.5\n",
        "    mu1 = cv2.GaussianBlur(img_a_gray, kernel, sigma)\n",
        "    mu2 = cv2.GaussianBlur(img_b_gray, kernel, sigma)\n",
        "    mu1_sq = mu1 * mu1\n",
        "    mu2_sq = mu2 * mu2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "\n",
        "    sigma1_sq = cv2.GaussianBlur(img_a_gray * img_a_gray, kernel, sigma) - mu1_sq\n",
        "    sigma2_sq = cv2.GaussianBlur(img_b_gray * img_b_gray, kernel, sigma) - mu2_sq\n",
        "    sigma12 = cv2.GaussianBlur(img_a_gray * img_b_gray, kernel, sigma) - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
        "    return float(ssim_map.mean())\n",
        "\n",
        "\n",
        "def compute_diff_heatmap(img_a: np.ndarray, img_b: np.ndarray, gain: float = 6.0) -> np.ndarray:\n",
        "    diff = (img_b.astype(np.float32) - img_a.astype(np.float32)) * gain + 128\n",
        "    diff = np.clip(diff, 0, 255).astype(np.uint8)\n",
        "    return diff\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "records = []\n",
        "for pair in all_pairs:\n",
        "    orig = load_rgb(pair[\"original_path\"])\n",
        "    wm = load_rgb(pair[\"watermarked_path\"])\n",
        "    psnr = compute_psnr(orig, wm)\n",
        "    ssim = compute_ssim(orig, wm)\n",
        "    records.append({\n",
        "        \"split\": pair[\"split\"],\n",
        "        \"image_id\": pair[\"image_id\"],\n",
        "        \"psnr\": psnr,\n",
        "        \"ssim\": ssim,\n",
        "        \"original_path\": pair[\"original_path\"],\n",
        "        \"watermarked_path\": pair[\"watermarked_path\"],\n",
        "    })\n",
        "\n",
        "metrics_df = pd.DataFrame(records)\n",
        "metrics_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_sample_triples(df: pd.DataFrame, n: int = 3):\n",
        "    subset = df.sample(min(n, len(df)), random_state=42)\n",
        "    fig, axes = plt.subplots(len(subset), 3, figsize=(14, 4 * len(subset)))\n",
        "    if len(subset) == 1:\n",
        "        axes = np.expand_dims(axes, axis=0)\n",
        "\n",
        "    for row_axes, (_, row) in zip(axes, subset.iterrows()):\n",
        "        orig = load_rgb(row[\"original_path\"])\n",
        "        wm = load_rgb(row[\"watermarked_path\"])\n",
        "        diff = compute_diff_heatmap(orig, wm, gain=8.0)\n",
        "\n",
        "        row_axes[0].imshow(orig)\n",
        "        row_axes[0].set_title(f\"Original\\n{row['image_id']}\")\n",
        "        row_axes[0].axis(\"off\")\n",
        "\n",
        "        row_axes[1].imshow(wm)\n",
        "        row_axes[1].set_title(f\"Watermarked\\nPSNR: {row['psnr']:.2f} dB, SSIM: {row['ssim']:.4f}\")\n",
        "        row_axes[1].axis(\"off\")\n",
        "\n",
        "        row_axes[2].imshow(diff)\n",
        "        row_axes[2].set_title(\"Difference (amplified)\")\n",
        "        row_axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "show_sample_triples(metrics_df, n=DISPLAY_SAMPLE_COUNT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PSNR_THRESHOLD = 30.0\n",
        "SSIM_THRESHOLD = 0.90\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "sns.histplot(data=metrics_df, x=\"psnr\", hue=\"split\", multiple=\"stack\", ax=axes[0, 0])\n",
        "axes[0, 0].axvline(PSNR_THRESHOLD, color=\"red\", linestyle=\"--\", label=f\"{PSNR_THRESHOLD} dB\")\n",
        "axes[0, 0].set_title(\"PSNR Distribution by Split\")\n",
        "axes[0, 0].legend()\n",
        "\n",
        "sns.histplot(data=metrics_df, x=\"ssim\", hue=\"split\", multiple=\"stack\", ax=axes[0, 1])\n",
        "axes[0, 1].axvline(SSIM_THRESHOLD, color=\"red\", linestyle=\"--\", label=f\"{SSIM_THRESHOLD} SSIM\")\n",
        "axes[0, 1].set_title(\"SSIM Distribution by Split\")\n",
        "axes[0, 1].legend()\n",
        "\n",
        "sns.boxplot(data=metrics_df, x=\"split\", y=\"psnr\", ax=axes[0, 2])\n",
        "axes[0, 2].axhline(PSNR_THRESHOLD, color=\"red\", linestyle=\"--\")\n",
        "axes[0, 2].set_title(\"PSNR Box Plot by Split\")\n",
        "\n",
        "sns.boxplot(data=metrics_df, x=\"split\", y=\"ssim\", ax=axes[1, 0])\n",
        "axes[1, 0].axhline(SSIM_THRESHOLD, color=\"red\", linestyle=\"--\")\n",
        "axes[1, 0].set_title(\"SSIM Box Plot by Split\")\n",
        "\n",
        "sns.scatterplot(data=metrics_df, x=\"psnr\", y=\"ssim\", hue=\"split\", ax=axes[1, 1], s=25, alpha=0.8)\n",
        "axes[1, 1].axvline(PSNR_THRESHOLD, color=\"red\", linestyle=\"--\")\n",
        "axes[1, 1].axhline(SSIM_THRESHOLD, color=\"red\", linestyle=\"--\")\n",
        "axes[1, 1].set_title(\"PSNR vs SSIM Correlation\")\n",
        "\n",
        "axes[1, 2].axis(\"off\")\n",
        "summary = metrics_df.groupby(\"split\").agg(\n",
        "    psnr_mean=(\"psnr\", \"mean\"),\n",
        "    psnr_std=(\"psnr\", \"std\"),\n",
        "    ssim_mean=(\"ssim\", \"mean\"),\n",
        "    ssim_std=(\"ssim\", \"std\"),\n",
        ")\n",
        "axes[1, 2].table(\n",
        "    cellText=[[f\"{row.psnr_mean:.2f} ± {row.psnr_std:.2f}\", f\"{row.ssim_mean:.3f} ± {row.ssim_std:.3f}\"] for row in summary.itertuples()],\n",
        "    rowLabels=summary.index,\n",
        "    colLabels=[\"PSNR (dB)\", \"SSIM\"],\n",
        "    loc=\"center\",\n",
        ")\n",
        "axes[1, 2].set_title(\"Summary Statistics\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "OUTPUT_METRICS_CSV = DATA_ROOT / \"metrics_summary.csv\"\n",
        "metrics_df.to_csv(OUTPUT_METRICS_CSV, index=False)\n",
        "print(f\"Metrics written to {OUTPUT_METRICS_CSV}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attack Robustness Visuals\n",
        "\n",
        "If you have already run `training/test_harness_det.py`, this section consumes the resulting `outputs/results.csv` file to show how each profile layer survives different attacks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if HARNESS_RESULTS_CSV.exists():\n",
        "    attacks_df = pd.read_csv(HARNESS_RESULTS_CSV)\n",
        "\n",
        "    def to_bool(val):\n",
        "        if pd.isna(val):\n",
        "            return np.nan\n",
        "        if isinstance(val, bool):\n",
        "            return val\n",
        "        return str(val).strip().lower() in {\"true\", \"1\", \"yes\", \"y\"}\n",
        "\n",
        "    attacks_df[\"decode_success\"] = attacks_df[\"decode_success\"].apply(to_bool)\n",
        "    attacks_df[\"bit_accuracy\"] = pd.to_numeric(attacks_df[\"bit_accuracy\"], errors=\"coerce\")\n",
        "    print(f\"Loaded {len(attacks_df)} attack evaluations from {HARNESS_RESULTS_CSV}\")\n",
        "else:\n",
        "    attacks_df = None\n",
        "    print(f\"⚠️ Harness CSV not found at {HARNESS_RESULTS_CSV}. Run the test harness first or update the path.\")\n",
        "    print(\"   Generate it via training/test_harness_det.py → TestHarness.run_batch(...)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if attacks_df is not None and not attacks_df.empty:\n",
        "    attack_summary = (\n",
        "        attacks_df.groupby([\"layer\", \"attack\"]).agg(\n",
        "            success_rate=(\"decode_success\", \"mean\"),\n",
        "            avg_bit_accuracy=(\"bit_accuracy\", \"mean\"),\n",
        "            samples=(\"decode_success\", \"size\"),\n",
        "        )\n",
        "    ).reset_index()\n",
        "\n",
        "    heatmap_data = attack_summary.pivot(index=\"attack\", columns=\"layer\", values=\"success_rate\").fillna(0)\n",
        "    plt.figure(figsize=(10, max(6, heatmap_data.shape[0] * 0.3)))\n",
        "    sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", vmin=0, vmax=1, cmap=\"YlGnBu\")\n",
        "    plt.title(\"Decode Success Rate by Attack / Layer\")\n",
        "    plt.xlabel(\"Layer\")\n",
        "    plt.ylabel(\"Attack\")\n",
        "    plt.tight_layout()\n",
        "else:\n",
        "    attack_summary = None\n",
        "    print(\"No attack data available to plot.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if attack_summary is not None and not attack_summary.empty:\n",
        "    semi_summary = (\n",
        "        attack_summary[attack_summary[\"layer\"] == \"SemiFragile\"]\n",
        "        .sort_values(\"avg_bit_accuracy\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(10, max(4, semi_summary.shape[0] * 0.3)))\n",
        "    sns.barplot(data=semi_summary, x=\"avg_bit_accuracy\", y=\"attack\", palette=\"magma\")\n",
        "    plt.title(\"Average Bit Accuracy per Attack (Semi-Fragile)\")\n",
        "    plt.xlabel(\"Bit Accuracy\")\n",
        "    plt.ylabel(\"Attack\")\n",
        "    plt.xlim(0, 1)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    print(\"Top attacks harming Semi-Fragile layer (by bit accuracy):\")\n",
        "    display(semi_summary.sort_values(\"avg_bit_accuracy\").head(5)[[\"attack\", \"avg_bit_accuracy\", \"success_rate\", \"samples\"]])\n",
        "\n",
        "    lsb_summary = attack_summary[attack_summary[\"layer\"] == \"LSB\"].sort_values(\"success_rate\")\n",
        "    print(\"\\nWorst-case attacks for LSB layer (ownership-proof):\")\n",
        "    display(lsb_summary.head(5)[[\"attack\", \"success_rate\", \"samples\"]])\n",
        "else:\n",
        "    print(\"No attack summary to visualize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- The histogram/box-plot thresholds (`PSNR_THRESHOLD`, `SSIM_THRESHOLD`) highlight the quality targets you care about—tweak them per project.\n",
        "- To inspect additional splits (e.g., `holdout`, `ablation`), just add them to `SPLITS` and rerun the first config cell.\n",
        "- `metrics_summary.csv` captures per-image stats so you can feed them into other dashboards or reports.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
